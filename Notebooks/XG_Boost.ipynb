{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "311ee5e5",
   "metadata": {},
   "source": [
    "## ðŸ§¬ TFBS Classification using XGBoost and k-mer Word2Vec Embeddings\n",
    "\n",
    "This notebook applies the XGBoost algorithm to classify DNA sequences as TFBS or non-TFBS using features derived from Word2Vec-encoded k-mers. Each DNA sequence is converted into a series of k-mers, which are then mapped to dense vectors using a pretrained Word2Vec model. These vectors are aggregated (e.g., via averaging) to form a fixed-length feature vector per sequence.\n",
    "\n",
    "XGBoost, a gradient-boosted tree-based ensemble model, is trained on these vectors for binary classification. The notebook includes performance evaluation metrics such as accuracy, precision, recall, F1-score, and ROC-AUC to assess model quality.\n",
    "\n",
    "This approach leverages the interpretability and speed of XGBoost, combined with the contextual power of distributed k-mer embeddings, to provide a high-performance baseline for TFBS prediction from DNA sequences.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f8c925",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "\n",
    "sys.path.append(\"../utils\")\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "\n",
    "from initialize_results_df import initialize_results_df\n",
    "\n",
    "from load_sequence_data import load_sequence_data\n",
    "\n",
    "\n",
    "# from xgb_kmer_utils import run_xgb_grid_search\n",
    "\n",
    "from xgb_kmer_utils import (\n",
    "    build_kmer_vocab,\n",
    "    build_vectorizer_from_vocab,\n",
    "    run_xgb_random_search,\n",
    "    get_kmers_stride,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e7ec04",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"..\\\\Data\"\n",
    "excel_dir = \"..\\\\Outputs\\\\excel_results.xlsx\"\n",
    "\n",
    "results_df, excel_df = initialize_results_df(data_dir, excel_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6278291",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = load_sequence_data(results_df[\"train_path\"][0])\n",
    "test_df = load_sequence_data(results_df[\"test_path\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7728e4db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Processing k=3\n",
      "âœ… Trial 1/30: acc=0.4977\n",
      "âœ… Trial 2/30: acc=0.5207\n",
      "âœ… Trial 3/30: acc=0.5011\n",
      "âœ… Trial 4/30: acc=0.4984\n",
      "âœ… Trial 5/30: acc=0.5576\n",
      "âœ… Trial 6/30: acc=0.4815\n",
      "âœ… Trial 7/30: acc=0.4994\n",
      "âœ… Trial 8/30: acc=0.5382\n",
      "âœ… Trial 9/30: acc=0.5015\n",
      "âœ… Trial 10/30: acc=0.5025\n",
      "âœ… Trial 11/30: acc=0.5009\n",
      "âœ… Trial 12/30: acc=0.5262\n",
      "âœ… Trial 13/30: acc=0.5550\n",
      "âœ… Trial 14/30: acc=0.4940\n",
      "âœ… Trial 15/30: acc=0.4806\n",
      "âœ… Trial 16/30: acc=0.5090\n",
      "âœ… Trial 17/30: acc=0.5260\n",
      "âœ… Trial 18/30: acc=0.5119\n",
      "âœ… Trial 19/30: acc=0.5004\n",
      "âœ… Trial 20/30: acc=0.5128\n",
      "âœ… Trial 21/30: acc=0.5262\n",
      "âœ… Trial 22/30: acc=0.5090\n",
      "âœ… Trial 23/30: acc=0.5036\n",
      "âœ… Trial 24/30: acc=0.5345\n",
      "âœ… Trial 25/30: acc=0.5142\n",
      "âœ… Trial 26/30: acc=0.5587\n",
      "âœ… Trial 27/30: acc=0.4984\n",
      "âœ… Trial 28/30: acc=0.5009\n",
      "âœ… Trial 29/30: acc=0.5413\n",
      "âœ… Trial 30/30: acc=0.5068\n",
      "ðŸŽ¯ Best Config:\n",
      "    trial  k  stride  n_estimators  max_depth  learning_rate  subsample  \\\n",
      "25     26  3       1          1000          6           0.01        0.8   \n",
      "4       5  3       1           100          6           0.10        0.6   \n",
      "12     13  3       2          1000         10           0.05        1.0   \n",
      "28     29  3       1           500          4           0.01        1.0   \n",
      "7       8  3       1           100          6           0.01        1.0   \n",
      "\n",
      "    colsample_bytree  gamma  accuracy  \n",
      "25               0.6    0.1  0.558737  \n",
      "4                0.8    0.1  0.557550  \n",
      "12               1.0    0.1  0.555022  \n",
      "28               0.8    1.0  0.541299  \n",
      "7                0.8    0.1  0.538152  \n",
      "âš¡ Total time: 206.37 seconds.\n",
      "âœ… Done k=3\n",
      "\n",
      "âœ… Processing k=5\n",
      "âœ… Trial 1/30: acc=0.5075\n",
      "âœ… Trial 2/30: acc=0.4955\n",
      "âœ… Trial 3/30: acc=0.6014\n",
      "âœ… Trial 4/30: acc=0.5079\n",
      "âœ… Trial 5/30: acc=0.5644\n",
      "âœ… Trial 6/30: acc=0.5921\n",
      "âœ… Trial 7/30: acc=0.5921\n",
      "âœ… Trial 8/30: acc=0.5079\n",
      "âœ… Trial 9/30: acc=0.5966\n",
      "âœ… Trial 10/30: acc=0.5845\n",
      "âœ… Trial 11/30: acc=0.5940\n",
      "âœ… Trial 12/30: acc=0.4991\n",
      "âœ… Trial 13/30: acc=0.5494\n",
      "âœ… Trial 14/30: acc=0.5803\n",
      "âœ… Trial 15/30: acc=0.4977\n",
      "âœ… Trial 16/30: acc=0.5734\n",
      "âœ… Trial 17/30: acc=0.4983\n",
      "âœ… Trial 18/30: acc=0.5947\n",
      "âœ… Trial 19/30: acc=0.5000\n",
      "âœ… Trial 20/30: acc=0.5012\n",
      "âœ… Trial 21/30: acc=0.5158\n",
      "âœ… Trial 22/30: acc=0.6095\n",
      "âœ… Trial 23/30: acc=0.5450\n",
      "âœ… Trial 24/30: acc=0.4977\n",
      "âœ… Trial 25/30: acc=0.6021\n",
      "âœ… Trial 26/30: acc=0.5220\n",
      "âœ… Trial 27/30: acc=0.5832\n",
      "âœ… Trial 28/30: acc=0.5937\n",
      "âœ… Trial 29/30: acc=0.4979\n",
      "âœ… Trial 30/30: acc=0.5017\n",
      "ðŸŽ¯ Best Config:\n",
      "    trial  k  stride  n_estimators  max_depth  learning_rate  subsample  \\\n",
      "21     22  5       2          1000          8           0.05        0.6   \n",
      "24     25  5       2           500          4           0.05        0.8   \n",
      "2       3  5       1           300         10           0.01        0.8   \n",
      "8       9  5       2           500          8           0.01        0.6   \n",
      "17     18  5       2           500          6           0.05        0.8   \n",
      "\n",
      "    colsample_bytree  gamma  accuracy  \n",
      "21               0.8    0.0  0.609503  \n",
      "24               0.6    1.0  0.602074  \n",
      "2                1.0    0.1  0.601403  \n",
      "8                0.8    0.0  0.596605  \n",
      "17               0.8    1.0  0.594696  \n",
      "âš¡ Total time: 561.97 seconds.\n",
      "âœ… Done k=5\n",
      "\n",
      "âœ… Processing k=6\n",
      "âœ… Trial 1/30: acc=0.5936\n",
      "âœ… Trial 2/30: acc=0.6037\n",
      "âœ… Trial 3/30: acc=0.5832\n",
      "âœ… Trial 4/30: acc=0.5742\n",
      "âœ… Trial 5/30: acc=0.6357\n",
      "âœ… Trial 6/30: acc=0.5962\n",
      "âœ… Trial 7/30: acc=0.5814\n",
      "âœ… Trial 8/30: acc=0.6995\n",
      "âœ… Trial 9/30: acc=0.6326\n",
      "âœ… Trial 10/30: acc=0.6066\n",
      "âœ… Trial 11/30: acc=0.6032\n",
      "âœ… Trial 12/30: acc=0.5716\n",
      "âœ… Trial 13/30: acc=0.5708\n",
      "âœ… Trial 14/30: acc=0.5912\n",
      "âœ… Trial 15/30: acc=0.6630\n",
      "âœ… Trial 16/30: acc=0.5831\n",
      "âœ… Trial 17/30: acc=0.6096\n",
      "âœ… Trial 18/30: acc=0.5680\n",
      "âœ… Trial 19/30: acc=0.5926\n",
      "âœ… Trial 20/30: acc=0.5614\n",
      "âœ… Trial 21/30: acc=0.6474\n",
      "âœ… Trial 22/30: acc=0.6392\n",
      "âœ… Trial 23/30: acc=0.5726\n",
      "âœ… Trial 24/30: acc=0.5744\n",
      "âœ… Trial 25/30: acc=0.5734\n",
      "âœ… Trial 26/30: acc=0.5974\n",
      "âœ… Trial 27/30: acc=0.6020\n",
      "âœ… Trial 28/30: acc=0.6114\n",
      "âœ… Trial 29/30: acc=0.5950\n",
      "âœ… Trial 30/30: acc=0.5834\n",
      "ðŸŽ¯ Best Config:\n",
      "    trial  k  stride  n_estimators  max_depth  learning_rate  subsample  \\\n",
      "7       8  6       1           500         10           0.05        0.8   \n",
      "14     15  6       1           100         10           0.05        0.6   \n",
      "20     21  6       1           300          8           0.05        1.0   \n",
      "21     22  6       1          1000          6           0.10        0.6   \n",
      "4       5  6       1           500          4           0.05        0.6   \n",
      "\n",
      "    colsample_bytree  gamma  accuracy  \n",
      "7                0.6    0.1  0.699531  \n",
      "14               0.8    0.0  0.663004  \n",
      "20               0.6    0.0  0.647371  \n",
      "21               0.6    0.0  0.639220  \n",
      "4                0.6    1.0  0.635712  \n",
      "âš¡ Total time: 1592.21 seconds.\n",
      "âœ… Done k=6\n",
      "\n",
      "ðŸŽ‰ Saved ALL k + stride results to xgb_random_results_all.xlsx\n"
     ]
    }
   ],
   "source": [
    "# Define XGBoost param grid\n",
    "xgb_param_grid = {\n",
    "    \"n_estimators\": [100, 300, 500, 1000],\n",
    "    \"max_depth\": [4, 6, 8, 10],\n",
    "    \"learning_rate\": [0.1, 0.05, 0.01],\n",
    "    \"subsample\": [0.6, 0.8, 1.0],\n",
    "    \"colsample_bytree\": [0.6, 0.8, 1.0],\n",
    "    \"gamma\": [0, 0.1, 1],\n",
    "}\n",
    "\n",
    "stride_values = [1, 2]\n",
    "use_tfidf = False\n",
    "\n",
    "\n",
    "# Store all result DataFrames here\n",
    "all_results = []\n",
    "\n",
    "# âœ… Loop over each k\n",
    "for k in [3, 5, 6]:\n",
    "    print(f\"âœ… Processing k={k}\")\n",
    "\n",
    "    # Build vocab and vectorizer for this k\n",
    "    vocab_dict = build_kmer_vocab(k)\n",
    "    vectorizer = build_vectorizer_from_vocab(vocab_dict, use_tfidf=use_tfidf)\n",
    "\n",
    "    # Run random search for this k\n",
    "    results_df = run_xgb_random_search(\n",
    "        train_df=train_df,\n",
    "        test_df=test_df,\n",
    "        k_values=[k],  # only this k\n",
    "        stride_values=stride_values,\n",
    "        xgb_param_grid=xgb_param_grid,\n",
    "        output_csv=f\"xgb_random_results_temp_k_{k}.csv\",  # temp CSV for backup\n",
    "        vectorizer=vectorizer,\n",
    "        n_trials=30,  # adjust number of trials\n",
    "    )\n",
    "\n",
    "    results_df[\"k\"] = k  # Add k column (in case not already present)\n",
    "    all_results.append(results_df)\n",
    "\n",
    "    print(f\"âœ… Done k={k}\\n\")\n",
    "\n",
    "# âœ… Combine all results into a single DataFrame\n",
    "final_df = pd.concat(all_results, ignore_index=True)\n",
    "\n",
    "# âœ… Save to single Excel file\n",
    "final_df.to_excel(\"../Outputs/random_search_xgb_cv.xlsx\", index=False)\n",
    "\n",
    "print(\"ðŸŽ‰ Saved ALL k + stride results to xgb_cv.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25532ba9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŽ¯ Best Config: k=6, stride=1, params={'n_estimators': 500, 'max_depth': 10, 'learning_rate': 0.05, 'subsample': 0.8, 'colsample_bytree': 0.6, 'gamma': 0.1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dhair\\Desktop\\Main_Project\\capstone_env\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [23:01:17] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\common\\error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\dhair\\Desktop\\Main_Project\\capstone_env\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [23:01:17] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"predictor\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Final model saved to ../Outputs/xgboost-cv.model\n",
      "âœ… Model summary saved to ../Outputs/xgboost-cv-summary.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dhair\\Desktop\\Main_Project\\capstone_env\\Lib\\site-packages\\xgboost\\sklearn.py:1028: UserWarning: [23:01:17] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\common\\error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  self.get_booster().save_model(fname)\n",
      "c:\\Users\\dhair\\Desktop\\Main_Project\\capstone_env\\Lib\\site-packages\\xgboost\\sklearn.py:1028: UserWarning: [23:01:18] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\c_api\\c_api.cc:1427: Saving model in the UBJSON format as default.  You can use file extension: `json`, `ubj` or `deprecated` to choose between formats.\n",
      "  self.get_booster().save_model(fname)\n"
     ]
    }
   ],
   "source": [
    "# âœ… Get best row\n",
    "best = final_df.sort_values(by=\"accuracy\", ascending=False).iloc[0]\n",
    "\n",
    "# âœ… Extract best params\n",
    "best_k = int(best[\"k\"])\n",
    "best_stride = int(best[\"stride\"])\n",
    "best_params = {\n",
    "    \"n_estimators\": int(best[\"n_estimators\"]),\n",
    "    \"max_depth\": int(best[\"max_depth\"]),\n",
    "    \"learning_rate\": float(best[\"learning_rate\"]),\n",
    "    \"subsample\": float(best[\"subsample\"]),\n",
    "    \"colsample_bytree\": float(best[\"colsample_bytree\"]),\n",
    "    \"gamma\": float(best[\"gamma\"]),\n",
    "}\n",
    "\n",
    "print(\n",
    "    f\"ðŸŽ¯ Best Config: k={best_k}, stride={best_stride}, params={best_params}\"\n",
    ")\n",
    "\n",
    "# âœ… Build vocab + vectorizer for best k\n",
    "vocab_dict = build_kmer_vocab(best_k)\n",
    "vectorizer = build_vectorizer_from_vocab(vocab_dict, use_tfidf=use_tfidf)\n",
    "\n",
    "# âœ… Recreate input features\n",
    "train_kmers = train_df[\"sequence\"].apply(\n",
    "    lambda seq: get_kmers_stride(seq, best_k, best_stride)\n",
    ")\n",
    "test_kmers = test_df[\"sequence\"].apply(\n",
    "    lambda seq: get_kmers_stride(seq, best_k, best_stride)\n",
    ")\n",
    "\n",
    "X_train = vectorizer.transform(train_kmers)\n",
    "X_test = vectorizer.transform(test_kmers)\n",
    "\n",
    "# âœ… Train final model\n",
    "model = xgb.XGBClassifier(\n",
    "    tree_method=\"gpu_hist\", predictor=\"gpu_predictor\", gpu_id=0, **best_params\n",
    ")\n",
    "model.fit(X_train, train_df[\"label\"])\n",
    "\n",
    "# âœ… Save final model\n",
    "model_output_path = \"../Models/xgboost-cv.model\"\n",
    "model.save_model(model_output_path)\n",
    "print(f\"âœ… Final model saved to {model_output_path}\")\n",
    "\n",
    "# âœ… Optional: save JSON summary\n",
    "import json\n",
    "\n",
    "summary = {\n",
    "    \"k\": best_k,\n",
    "    \"stride\": best_stride,\n",
    "    \"accuracy\": round(float(best[\"accuracy\"]), 4),\n",
    "    \"xgboost_params\": best_params,\n",
    "}\n",
    "\n",
    "json_output_path = \"../Models/xgboost-cv-summary.json\"\n",
    "with open(json_output_path, \"w\") as f:\n",
    "    json.dump(summary, f, indent=4)\n",
    "print(f\"âœ… Model summary saved to {json_output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c0778ec",
   "metadata": {},
   "source": [
    "# LOOPING THROUGH FOLDERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aac401e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "\n",
    "sys.path.append(\"../utils\")\n",
    "\n",
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "\n",
    "import json\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    roc_auc_score,\n",
    "    average_precision_score,\n",
    ")\n",
    "\n",
    "from xgb_kmer_utils import (\n",
    "    build_kmer_vocab,\n",
    "    build_vectorizer_from_vocab,\n",
    "    get_kmers_str,\n",
    ")\n",
    "\n",
    "from load_sequence_data import load_sequence_data\n",
    "\n",
    "from initialize_results_df import initialize_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a764c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "data_dir = \"../Data\"\n",
    "excel_path = \"../Outputs/50_XGBOOST_CV.xlsx\"\n",
    "model_path = \"../Models/xgboost-cv.model\"\n",
    "\n",
    "# Load dataframes\n",
    "results_df, excel_df = initialize_results_df(data_dir, excel_path)\n",
    "\n",
    "# âœ… Load JSON best params\n",
    "with open(\"../Models/xgboost-cv-summary.json\", \"r\") as f:\n",
    "    summary = json.load(f)\n",
    "\n",
    "k = summary[\"k\"]\n",
    "stride = summary[\"stride\"]\n",
    "best_params = summary[\"xgboost_params\"]\n",
    "\n",
    "# âœ… Add refresh update params\n",
    "best_params.update(\n",
    "    {\n",
    "        \"process_type\": \"update\",\n",
    "        \"updater\": \"refresh\",\n",
    "        \"tree_method\": \"hist\",  # CPU only\n",
    "        \"predictor\": \"cpu_predictor\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a79175",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_dict = build_kmer_vocab(k)\n",
    "vectorizer = build_vectorizer_from_vocab(vocab_dict, use_tfidf=False)\n",
    "\n",
    "model = xgb.XGBClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c180f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Processing folder: wgEncodeAwgTfbsBroadDnd41CtcfUniPk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dhair\\Desktop\\Main_Project\\capstone_env\\Lib\\site-packages\\xgboost\\sklearn.py:1028: UserWarning: [23:47:15] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\c_api\\c_api.cc:1427: Saving model in the UBJSON format as default.  You can use file extension: `json`, `ubj` or `deprecated` to choose between formats.\n",
      "  self.get_booster().save_model(fname)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… wgEncodeAwgTfbsBroadDnd41CtcfUniPk: train_acc=0.5023, test_acc=0.5009\n",
      "âœ… Processing folder: wgEncodeAwgTfbsBroadDnd41Ezh239875UniPk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dhair\\Desktop\\Main_Project\\capstone_env\\Lib\\site-packages\\xgboost\\sklearn.py:1028: UserWarning: [23:47:16] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\c_api\\c_api.cc:1427: Saving model in the UBJSON format as default.  You can use file extension: `json`, `ubj` or `deprecated` to choose between formats.\n",
      "  self.get_booster().save_model(fname)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… wgEncodeAwgTfbsBroadDnd41Ezh239875UniPk: train_acc=0.5049, test_acc=0.4814\n",
      "âœ… Processing folder: wgEncodeAwgTfbsBroadGm12878CtcfUniPk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dhair\\Desktop\\Main_Project\\capstone_env\\Lib\\site-packages\\xgboost\\sklearn.py:1028: UserWarning: [23:47:24] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\c_api\\c_api.cc:1427: Saving model in the UBJSON format as default.  You can use file extension: `json`, `ubj` or `deprecated` to choose between formats.\n",
      "  self.get_booster().save_model(fname)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… wgEncodeAwgTfbsBroadGm12878CtcfUniPk: train_acc=0.5018, test_acc=0.5020\n",
      "âœ… Processing folder: wgEncodeAwgTfbsBroadGm12878Ezh239875UniPk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dhair\\Desktop\\Main_Project\\capstone_env\\Lib\\site-packages\\xgboost\\sklearn.py:1028: UserWarning: [23:47:24] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\c_api\\c_api.cc:1427: Saving model in the UBJSON format as default.  You can use file extension: `json`, `ubj` or `deprecated` to choose between formats.\n",
      "  self.get_booster().save_model(fname)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… wgEncodeAwgTfbsBroadGm12878Ezh239875UniPk: train_acc=0.5020, test_acc=0.4919\n",
      "âœ… Processing folder: wgEncodeAwgTfbsBroadH1hescChd1a301218aUniPk\n",
      "âœ… wgEncodeAwgTfbsBroadH1hescChd1a301218aUniPk: train_acc=0.5014, test_acc=0.4941\n",
      "âœ… Metrics saved to ../Outputs/50_XGBOOST_CV.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dhair\\Desktop\\Main_Project\\capstone_env\\Lib\\site-packages\\xgboost\\sklearn.py:1028: UserWarning: [23:47:26] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\c_api\\c_api.cc:1427: Saving model in the UBJSON format as default.  You can use file extension: `json`, `ubj` or `deprecated` to choose between formats.\n",
      "  self.get_booster().save_model(fname)\n"
     ]
    }
   ],
   "source": [
    "# âœ… Process first 50 folders\n",
    "for idx, row in results_df.iloc[:5].iterrows():\n",
    "    train_path = row[\"train_path\"]\n",
    "    test_path = row[\"test_path\"]\n",
    "    folder_name = row[\"folder_name\"]\n",
    "\n",
    "    print(f\"âœ… Processing folder: {folder_name}\")\n",
    "\n",
    "    model.load_model(model_path)\n",
    "\n",
    "    # Load data\n",
    "    train_df = load_sequence_data(train_path)\n",
    "    test_df = load_sequence_data(test_path)\n",
    "\n",
    "    # Convert sequences to k-mer strings\n",
    "    train_kmers = train_df[\"sequence\"].apply(\n",
    "        lambda seq: get_kmers_str(seq, k, stride)\n",
    "    )\n",
    "    test_kmers = test_df[\"sequence\"].apply(\n",
    "        lambda seq: get_kmers_str(seq, k, stride)\n",
    "    )\n",
    "\n",
    "    # Transform using same vectorizer\n",
    "    X_train = vectorizer.transform(train_kmers)\n",
    "    X_test = vectorizer.transform(test_kmers)\n",
    "\n",
    "    y_train = train_df[\"label\"]\n",
    "    y_test = test_df[\"label\"]\n",
    "\n",
    "    # âœ… Continue training model\n",
    "    model.fit(X_train, y_train, xgb_model=model)\n",
    "\n",
    "    # Evaluate train\n",
    "    preds_train = model.predict(X_train)\n",
    "    train_acc = accuracy_score(y_train, preds_train)\n",
    "    train_proba = model.predict_proba(X_train)[:, 1]\n",
    "    train_pr_auc = average_precision_score(y_train, train_proba)\n",
    "    train_roc_auc = roc_auc_score(y_train, train_proba)\n",
    "\n",
    "    # Evaluate test\n",
    "    preds_test = model.predict(X_test)\n",
    "    test_acc = accuracy_score(y_test, preds_test)\n",
    "    test_proba = model.predict_proba(X_test)[:, 1]\n",
    "    test_pr_auc = average_precision_score(y_test, test_proba)\n",
    "    test_roc_auc = roc_auc_score(y_test, test_proba)\n",
    "\n",
    "    # âœ… Save metrics to excel_df\n",
    "    excel_df.at[idx, \"train_accuracy\"] = train_acc\n",
    "    excel_df.at[idx, \"test_accuracy\"] = test_acc\n",
    "    excel_df.at[idx, \"pr-roc\"] = test_roc_auc\n",
    "    excel_df.at[idx, \"pr-auc\"] = test_pr_auc\n",
    "\n",
    "    # âœ… Save updated model after each folder\n",
    "    model.save_model(model_path)\n",
    "\n",
    "    print(\n",
    "        f\"âœ… {folder_name}: train_acc={train_acc:.4f}, test_acc={test_acc:.4f}\"\n",
    "    )\n",
    "\n",
    "# âœ… Save updated Excel\n",
    "excel_df.to_excel(excel_path, index=False)\n",
    "print(f\"âœ… Metrics saved to {excel_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b089205",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "capstone_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
